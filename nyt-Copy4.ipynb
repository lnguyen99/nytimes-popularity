{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "clinical-cursor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import re\n",
    "import datetime as dt\n",
    "\n",
    "import keras as ks\n",
    "import tensorflow as tf\n",
    "\n",
    "import sklearn\n",
    "import sklearn.preprocessing\n",
    "import sklearn.model_selection\n",
    "import sklearn.neighbors\n",
    "import sklearn.ensemble\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.feature_extraction.text import HashingVectorizer, TfidfVectorizer, CountVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understood-morocco",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "artificial-intelligence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_popular_words(corpus, top_n):\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    feature_array = vectorizer.get_feature_names()\n",
    "    top_words = sorted(list(zip(vectorizer.get_feature_names(), X.sum(0).getA1())), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    result = [x[0] for x in top_words]\n",
    "    print(top_words)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "regulated-rescue",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pw_from_file(filename, column, n_top):\n",
    "    df = pd.read_csv(filename)\n",
    "    df['all_text'] = df['headline'] + \" \" + df['abstract'] + \" \" + df['keywords']\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    df[column] = df[column].astype(str)\n",
    "    df = lemmatize_column(df, column, lemmatizer, stop_words)\n",
    "    popular_words = get_popular_words(df[column], n_top)\n",
    "    \n",
    "    return popular_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "collective-christianity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentence(sentence, lemmatizer, stop_words):\n",
    "    sentence = sentence.lower()\n",
    "    tokens = list(set(word_tokenize(sentence)))   \n",
    "    words = [lemmatizer.lemmatize(word) for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "metric-premium",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ord_encode(df, ordinal_features):\n",
    "    # Ordinal encode all of these features\n",
    "    ordinal = sklearn.preprocessing.OrdinalEncoder()\n",
    "    df[ordinal_features] = ordinal.fit_transform(df[ordinal_features])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "enabling-monthly",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_language_column(df, col_name, popular_words = []):\n",
    "    vectorizer = CountVectorizer()\n",
    "    nc = vectorizer.fit_transform(df[col_name])\n",
    "    encoded_col = pd.DataFrame(nc.A, columns=vectorizer.get_feature_names())[popular_words]\n",
    "    df = pd.concat([df.reset_index(drop=True), encoded_col.reset_index(drop=True)], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "operating-murray",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_column(df, col_name, lemmatizer, stop_words):\n",
    "    df[col_name] = df[col_name].map(lambda x: process_sentence(x, lemmatizer, stop_words))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aggressive-lawsuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_and_preprocess(filename):\n",
    "    df = pd.read_csv(filename)    \n",
    "    \n",
    "    # create 3 new columns\n",
    "    df['week_day'] = df['pub_date'].map(lambda x: pd.Timestamp.to_pydatetime(pd.Timestamp(x)).weekday())\n",
    "    df['pub_hour'] = df['pub_date'].map(lambda x: pd.Timestamp.to_pydatetime(pd.Timestamp(x)).hour)\n",
    "    df['all_text'] = df['headline'] + \" \" + df['abstract'] + \" \" + df['keywords']\n",
    "    \n",
    "    # ordinal encode\n",
    "    df = ord_encode(df, ['newsdesk', 'section', 'material'])\n",
    "    \n",
    "    df = df.drop(['uniqueID', 'subsection', 'pub_date', 'headline', 'abstract', 'keywords'], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "rapid-diabetes",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_column(df, column, n_top, popular_words = []):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    df[column] = df[column].astype(str)\n",
    "    df = lemmatize_column(df, column, lemmatizer, stop_words)\n",
    "    if len(popular_words) == 0:\n",
    "        popular_words = get_popular_words(df[column], n_top)\n",
    "    df = encode_language_column(df, column, popular_words)\n",
    "    df = df.drop([column], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "rocky-brush",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_optimize_DT(X_train, y_train):\n",
    "    # REWRITE AS NEEDED FOR DIFFERENT MODELS\n",
    "    \n",
    "    # Cross-validation folds\n",
    "    k = 10\n",
    "\n",
    "    # Hyperparameters to tune:\n",
    "    params = {'min_samples_split': [2, 5, 10],\n",
    "             'criterion': ['gini', 'entropy'],\n",
    "              'max_depth': [5, 10, 20],\n",
    "              'min_samples_leaf': [1, 2, 5, 10]\n",
    "             }\n",
    "    \n",
    "    # Initialize GridSearchCV object with decision tree classifier and hyperparameters\n",
    "    grid_tree = sklearn.model_selection.GridSearchCV(estimator=sklearn.tree.DecisionTreeClassifier(),\n",
    "                             param_grid=params,\n",
    "                             cv=k,\n",
    "                             return_train_score=True,\n",
    "                             scoring='accuracy',\n",
    "                             refit='accuracy') \n",
    "\n",
    "    # Train and cross-validate, print results\n",
    "    grid_tree.fit(X_train, y_train)\n",
    "\n",
    "    best_hyperparams = grid_tree.best_params_\n",
    "\n",
    "    # print best hyperparameters\n",
    "    print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "supposed-framework",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_optimize_KNN(X_train, y_train):\n",
    "    # REWRITE AS NEEDED FOR DIFFERENT MODELS\n",
    "    \n",
    "    # Cross-validation folds\n",
    "    k = 10\n",
    "\n",
    "    # Hyperparameters to tune:\n",
    "    params = {'n_neighbors': [3, 5, 8, 10, 15],\n",
    "                'weights': ['uniform', 'distance'],\n",
    "             }\n",
    "    \n",
    "    # Initialize GridSearchCV object with decision tree classifier and hyperparameters\n",
    "    grid_tree = sklearn.model_selection.GridSearchCV(estimator=sklearn.neighbors.KNeighborsClassifier(),\n",
    "                             param_grid=params,\n",
    "                             cv=k,\n",
    "                             return_train_score=True,\n",
    "                             scoring='accuracy',\n",
    "                             refit='accuracy') \n",
    "\n",
    "    # Train and cross-validate, print results\n",
    "    grid_tree.fit(X_train, y_train)\n",
    "\n",
    "    best_hyperparams = grid_tree.best_params_\n",
    "\n",
    "    # print best hyperparameters\n",
    "    print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "nasty-pocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_optimize_SVM(X_train, y_train):\n",
    "    # REWRITE AS NEEDED FOR DIFFERENT MODELS\n",
    "    \n",
    "    # Cross-validation folds\n",
    "    k = 10\n",
    "\n",
    "    # Hyperparameters to tune:\n",
    "    params = {'kernel': ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'],\n",
    "             'decision_function_shape': ['ovo', 'ovr']\n",
    "             }\n",
    "    \n",
    "    # Initialize GridSearchCV object with decision tree classifier and hyperparameters\n",
    "    grid_tree = sklearn.model_selection.GridSearchCV(estimator=sklearn.svm.SVC(),\n",
    "                             param_grid=params,\n",
    "                             cv=k,\n",
    "                             return_train_score=True,\n",
    "                             scoring='accuracy',\n",
    "                             refit='accuracy') \n",
    "\n",
    "    # Train and cross-validate, print results\n",
    "    grid_tree.fit(X_train, y_train)\n",
    "\n",
    "    best_hyperparams = grid_tree.best_params_\n",
    "\n",
    "    # print best hyperparameters\n",
    "    print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-hometown",
   "metadata": {},
   "source": [
    "# Start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "saved-shade",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top = 150\n",
    "text_column_to_change = 'all_text'\n",
    "df = open_and_preprocess(\"train.csv\")\n",
    "df = process_column(df, text_column_to_change, n_top)\n",
    "y_train = df['is_popular']\n",
    "# NOTE: REMOVING word_count DRASTICALLY IMPROVES ACCURACY\n",
    "X_train = df.drop(['is_popular', 'n_comments', 'word_count'], axis=1)\n",
    "ts = open_and_preprocess(\"test.csv\")\n",
    "ts = process_column(ts, text_column_to_change, n_top, get_pw_from_file('train.csv', text_column_to_change, n_top))\n",
    "y_test = ts['is_popular']\n",
    "X_test = ts.drop(['is_popular', 'word_count'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-messaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_optimize_DT(X_train, y_train)\n",
    "# base_optimize_KNN(X_train, y_train)\n",
    "base_optimize_SVM(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eastern-dialogue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6823529411764706\n"
     ]
    }
   ],
   "source": [
    "# PREDICTING WITH KNN\n",
    "knn = sklearn.neighbors.KNeighborsClassifier(n_neighbors=15, weights='distance')\n",
    "knn.fit(X_train, y_train)\n",
    "y_test_pred = knn.predict(X_test)\n",
    "accuracy = sklearn.metrics.accuracy_score(y_test, y_test_pred)\n",
    "print(accuracy)\n",
    "\n",
    "# only headline: {'n_neighbors': 15, 'weights': 'uniform'}\n",
    "# all_text: {'n_neighbors': 15, 'weights': 'distance'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "prescription-hungary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6645807259073843\n"
     ]
    }
   ],
   "source": [
    "# PREDICTING WITH DECISION TREE\n",
    "dt = sklearn.tree.DecisionTreeClassifier(criterion='entropy', max_depth=10, min_samples_leaf=10, min_samples_split=2)\n",
    "dt.fit(X_train, y_train)\n",
    "y_test_pred = dt.predict(X_test)\n",
    "accuracy = sklearn.metrics.accuracy_score(y_test, y_test_pred)\n",
    "print(accuracy)\n",
    "\n",
    "# only headline: {'criterion': 'entropy', 'max_depth': 20, 'min_samples_leaf': 10, 'min_samples_split': 5}\n",
    "# all_text: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 10, 'min_samples_split': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "wireless-weapon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6795994993742178\n"
     ]
    }
   ],
   "source": [
    "# PREDICTING WITH SVM\n",
    "svm = sklearn.svm.SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "y_test_pred = svm.predict(X_test)\n",
    "accuracy = sklearn.metrics.accuracy_score(y_test, y_test_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "threaded-playing",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "canadian-explanation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "90/90 [==============================] - 0s 3ms/step - loss: 0.6421 - accuracy: 0.6465 - val_loss: 0.5693 - val_accuracy: 0.7070\n",
      "Epoch 2/30\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.5518 - accuracy: 0.7154 - val_loss: 0.5512 - val_accuracy: 0.7234\n",
      "Epoch 3/30\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.5210 - accuracy: 0.7366 - val_loss: 0.5399 - val_accuracy: 0.7164\n",
      "Epoch 4/30\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4980 - accuracy: 0.7526 - val_loss: 0.5333 - val_accuracy: 0.7125\n",
      "Epoch 5/30\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4968 - accuracy: 0.7515 - val_loss: 0.5314 - val_accuracy: 0.7195\n",
      "Epoch 6/30\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4739 - accuracy: 0.7669 - val_loss: 0.5332 - val_accuracy: 0.7289\n",
      "Epoch 7/30\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4660 - accuracy: 0.7722 - val_loss: 0.5573 - val_accuracy: 0.7297\n",
      "Epoch 8/30\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4549 - accuracy: 0.7782 - val_loss: 0.5362 - val_accuracy: 0.7258\n",
      "Epoch 9/30\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4445 - accuracy: 0.7848 - val_loss: 0.5277 - val_accuracy: 0.7414\n",
      "Epoch 10/30\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4329 - accuracy: 0.7905 - val_loss: 0.5612 - val_accuracy: 0.7258\n",
      "Epoch 11/30\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4194 - accuracy: 0.7973 - val_loss: 0.5567 - val_accuracy: 0.7250\n",
      "Epoch 12/30\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4139 - accuracy: 0.8020 - val_loss: 0.5645 - val_accuracy: 0.7336\n",
      "Epoch 13/30\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3989 - accuracy: 0.8121 - val_loss: 0.5738 - val_accuracy: 0.7312\n",
      "Epoch 14/30\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3939 - accuracy: 0.8167 - val_loss: 0.5536 - val_accuracy: 0.7242\n",
      "Epoch 15/30\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3909 - accuracy: 0.8217 - val_loss: 0.5638 - val_accuracy: 0.7289\n",
      "Epoch 16/30\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3655 - accuracy: 0.8302 - val_loss: 0.6243 - val_accuracy: 0.7234\n",
      "Epoch 17/30\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3619 - accuracy: 0.8292 - val_loss: 0.6186 - val_accuracy: 0.7164\n",
      "Epoch 18/30\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3519 - accuracy: 0.8370 - val_loss: 0.6686 - val_accuracy: 0.7305\n",
      "Epoch 19/30\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3385 - accuracy: 0.8469 - val_loss: 0.6643 - val_accuracy: 0.7273\n",
      "Epoch 20/30\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3359 - accuracy: 0.8436 - val_loss: 0.7006 - val_accuracy: 0.7125\n",
      "Epoch 21/30\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3223 - accuracy: 0.8535 - val_loss: 0.7118 - val_accuracy: 0.7227\n",
      "Epoch 22/30\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3138 - accuracy: 0.8575 - val_loss: 0.7350 - val_accuracy: 0.7164\n",
      "Epoch 23/30\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3024 - accuracy: 0.8621 - val_loss: 0.7864 - val_accuracy: 0.7320\n",
      "Epoch 24/30\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2925 - accuracy: 0.8669 - val_loss: 0.7845 - val_accuracy: 0.7266\n",
      "Epoch 25/30\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2880 - accuracy: 0.8707 - val_loss: 0.8200 - val_accuracy: 0.7188\n",
      "Epoch 26/30\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2681 - accuracy: 0.8785 - val_loss: 0.7913 - val_accuracy: 0.7266\n",
      "Epoch 27/30\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2667 - accuracy: 0.8814 - val_loss: 0.8360 - val_accuracy: 0.7227\n",
      "Epoch 28/30\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2544 - accuracy: 0.8841 - val_loss: 0.8414 - val_accuracy: 0.7312\n",
      "Epoch 29/30\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2482 - accuracy: 0.8886 - val_loss: 0.9382 - val_accuracy: 0.7320\n",
      "Epoch 30/30\n",
      "90/90 [==============================] - 0s 3ms/step - loss: 0.2564 - accuracy: 0.8826 - val_loss: 0.9000 - val_accuracy: 0.7289\n",
      "The test accuracy is 0.7294117647058823\n"
     ]
    }
   ],
   "source": [
    "model = ks.models.Sequential()\n",
    "model.add(ks.layers.Flatten(input_shape=[shape]))\n",
    "model.add(ks.layers.Dense(256, activation=\"relu\"))\n",
    "model.add(ks.layers.Dense(128, activation=\"relu\"))\n",
    "model.add(ks.layers.Dense(64, activation=\"relu\"))\n",
    "model.add(ks.layers.Dense(32, activation=\"relu\"))\n",
    "model.add(ks.layers.Dense(2, activation=\"softmax\"))\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.fit(X_train, y_train, batch_size=128, epochs=30, validation_split=0.1)\n",
    "test_predictions = np.argmax(model.predict(X_test), axis=1)\n",
    "test_accuracy = metrics.accuracy_score(y_test, test_predictions)\n",
    "print(f\"The test accuracy is {test_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
