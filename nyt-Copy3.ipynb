{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fitting-strength",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import re\n",
    "import datetime as dt\n",
    "\n",
    "import keras as ks\n",
    "import tensorflow as tf\n",
    "\n",
    "import sklearn\n",
    "import sklearn.preprocessing\n",
    "import sklearn.model_selection\n",
    "import sklearn.neighbors\n",
    "import sklearn.ensemble\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "wicked-observer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_popular_words(corpus, top_n):\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    feature_array = vectorizer.get_feature_names()\n",
    "    top_words = sorted(list(zip(vectorizer.get_feature_names(), X.sum(0).getA1())), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    result = [x[0] for x in top_words]\n",
    "    print(top_words)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "narrow-artist",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pw_from_file(filename, column, n_top):\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    df[column] = df[column].astype(str)\n",
    "    df = lemmatize_column(df, column, lemmatizer, stop_words)\n",
    "    popular_words = get_popular_words(df[column], n_top)\n",
    "    \n",
    "    return popular_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "connected-catering",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentence(sentence, lemmatizer, stop_words):\n",
    "    sentence = sentence.lower()\n",
    "    tokens = list(set(word_tokenize(sentence)))   \n",
    "    words = [lemmatizer.lemmatize(word) for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "protective-abortion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ord_encode(df, ordinal_features):\n",
    "    # Ordinal encode all of these features\n",
    "    ordinal = sklearn.preprocessing.OrdinalEncoder()\n",
    "    df[ordinal_features] = ordinal.fit_transform(df[ordinal_features])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "subjective-desert",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_language_column(df, col_name, popular_words = []):\n",
    "    vectorizer = CountVectorizer()\n",
    "    nc = vectorizer.fit_transform(df[col_name])\n",
    "    encoded_col = pd.DataFrame(nc.A, columns=vectorizer.get_feature_names())[popular_words]\n",
    "    df = pd.concat([df.reset_index(drop=True), encoded_col.reset_index(drop=True)], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "disciplinary-fraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_column(df, col_name, lemmatizer, stop_words):\n",
    "    df[col_name] = df[col_name].map(lambda x: process_sentence(x, lemmatizer, stop_words))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "junior-virginia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_and_preprocess(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    # create 2 new columns\n",
    "    df['week_day'] = df['pub_date'].map(lambda x: pd.Timestamp.to_pydatetime(pd.Timestamp(x)).weekday())\n",
    "    df['pub_hour'] = df['pub_date'].map(lambda x: pd.Timestamp.to_pydatetime(pd.Timestamp(x)).hour)\n",
    "    # ordinal encode\n",
    "    df = ord_encode(df, ['newsdesk', 'section', 'material'])\n",
    "    \n",
    "    df = df.drop(['uniqueID', 'subsection', 'pub_date', 'headline', 'abstract'], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "irish-sheffield",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_column(df, column, n_top, popular_words = []):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    df[column] = df[column].astype(str)\n",
    "    df = lemmatize_column(df, column, lemmatizer, stop_words)\n",
    "    if len(popular_words) == 0:\n",
    "        popular_words = get_popular_words(df[column], n_top)\n",
    "    df = encode_language_column(df, column, popular_words)\n",
    "    df = df.drop([column], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "still-broadway",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('state', 3521), ('government', 2529), ('donald', 2080), ('politics', 2015), ('election', 1496), ('york', 1116), ('life', 1075), ('party', 1069), ('city', 870), ('jr', 823), ('people', 706), ('joseph', 686), ('estate', 669), ('ny', 638), ('housing', 632), ('protest', 620), ('residential', 613), ('service', 556), ('medium', 553), ('culture', 547), ('department', 546), ('reopenings', 539), ('movement', 537), ('program', 531), ('job', 516), ('institutional', 512), ('economy', 499), ('relation', 468), ('floyd', 444), ('literature', 421), ('book', 414), ('ethnicity', 411), ('tv', 410), ('medical', 404), ('international', 389), ('fatality', 376), ('nyc', 362), ('health', 360), ('security', 359), ('girl', 342), ('university', 338), ('puzzle', 337), ('misconduct', 333), ('public', 332), ('matter', 329), ('shooting', 323), ('act', 322), ('caucus', 322), ('manhattan', 317), ('force', 316), ('military', 289), ('internet', 288), ('cookbook', 286), ('riot', 286), ('bernard', 285), ('disease', 284), ('national', 277), ('aid', 275), ('economic', 275), ('brutality', 272), ('defense', 269), ('childhood', 265), ('john', 264), ('rate', 255), ('andrew', 254), ('control', 253), ('right', 249), ('michael', 248), ('safety', 248), ('opinion', 245), ('condition', 240), ('calif', 238), ('vacation', 237), ('care', 236), ('trend', 236), ('insurance', 233), ('group', 227), ('violation', 221), ('news', 220), ('area', 214), ('relief', 209), ('representative', 209), ('court', 207), ('trade', 207), ('music', 204), ('assn', 203), ('family', 203), ('time', 194), ('reduction', 191), ('journal', 190), ('world', 189), ('china', 188), ('play', 185), ('prevention', 185), ('warming', 185), ('movie', 180), ('concern', 179), ('stephen', 179), ('elizabeth', 177), ('war', 173)]\n"
     ]
    }
   ],
   "source": [
    "df = open_and_preprocess(\"train.csv\")\n",
    "df = process_column(df, 'keywords', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "controlling-processing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>newsdesk</th>\n",
       "      <th>section</th>\n",
       "      <th>material</th>\n",
       "      <th>word_count</th>\n",
       "      <th>is_popular</th>\n",
       "      <th>n_comments</th>\n",
       "      <th>week_day</th>\n",
       "      <th>pub_hour</th>\n",
       "      <th>state</th>\n",
       "      <th>government</th>\n",
       "      <th>...</th>\n",
       "      <th>world</th>\n",
       "      <th>china</th>\n",
       "      <th>play</th>\n",
       "      <th>prevention</th>\n",
       "      <th>warming</th>\n",
       "      <th>movie</th>\n",
       "      <th>concern</th>\n",
       "      <th>stephen</th>\n",
       "      <th>elizabeth</th>\n",
       "      <th>war</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>680</td>\n",
       "      <td>1</td>\n",
       "      <td>186</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>931</td>\n",
       "      <td>1</td>\n",
       "      <td>257</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1057</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1156</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12787</th>\n",
       "      <td>14.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1297</td>\n",
       "      <td>1</td>\n",
       "      <td>143</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12788</th>\n",
       "      <td>34.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12789</th>\n",
       "      <td>11.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>991</td>\n",
       "      <td>1</td>\n",
       "      <td>1516</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12790</th>\n",
       "      <td>39.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1709</td>\n",
       "      <td>1</td>\n",
       "      <td>702</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12791</th>\n",
       "      <td>39.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1580</td>\n",
       "      <td>1</td>\n",
       "      <td>1313</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12792 rows × 108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       newsdesk  section  material  word_count  is_popular  n_comments  \\\n",
       "0          11.0     17.0       0.0         680           1         186   \n",
       "1          15.0      7.0       4.0         931           1         257   \n",
       "2          41.0     22.0       4.0        1057           0           6   \n",
       "3          41.0     22.0       1.0           0           0           2   \n",
       "4          41.0     22.0       4.0        1156           0          25   \n",
       "...         ...      ...       ...         ...         ...         ...   \n",
       "12787      14.0     39.0       4.0        1297           1         143   \n",
       "12788      34.0     17.0       7.0          88           0          33   \n",
       "12789      11.0     17.0       0.0         991           1        1516   \n",
       "12790      39.0     35.0       4.0        1709           1         702   \n",
       "12791      39.0     35.0       4.0        1580           1        1313   \n",
       "\n",
       "       week_day  pub_hour  state  government  ...  world  china  play  \\\n",
       "0             2         0      0           0  ...      0      0     0   \n",
       "1             2         3      0           0  ...      0      0     0   \n",
       "2             2         5      0           0  ...      0      0     0   \n",
       "3             2         5      0           0  ...      0      0     0   \n",
       "4             2         5      0           0  ...      0      0     0   \n",
       "...         ...       ...    ...         ...  ...    ...    ...   ...   \n",
       "12787         2        18      0           1  ...      0      0     0   \n",
       "12788         2        20      0           0  ...      0      0     0   \n",
       "12789         2        21      0           0  ...      0      0     0   \n",
       "12790         2        23      1           1  ...      0      0     0   \n",
       "12791         2        23      1           1  ...      0      0     0   \n",
       "\n",
       "       prevention  warming  movie  concern  stephen  elizabeth  war  \n",
       "0               0        0      0        0        0          0    0  \n",
       "1               0        0      0        0        0          0    0  \n",
       "2               0        0      0        0        0          0    0  \n",
       "3               0        0      0        0        0          0    0  \n",
       "4               0        0      0        0        0          0    0  \n",
       "...           ...      ...    ...      ...      ...        ...  ...  \n",
       "12787           0        0      0        0        0          0    0  \n",
       "12788           0        0      0        0        0          0    0  \n",
       "12789           0        0      0        0        0          0    0  \n",
       "12790           0        0      0        0        0          0    0  \n",
       "12791           0        0      0        0        0          0    0  \n",
       "\n",
       "[12792 rows x 108 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "broken-trainer",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['is_popular']\n",
    "# NOTE: REMOVING word_count DRASTICALLY IMPROVES ACCURACY\n",
    "X = df.drop(['is_popular', 'n_comments', 'word_count'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "otherwise-delaware",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12792, 105)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "split-invalid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.738569753810082\n"
     ]
    }
   ],
   "source": [
    "# PREDICTING WITH KNN\n",
    "\n",
    "# 80/20 train/test split\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "\n",
    "# create classifiers\n",
    "knn = sklearn.neighbors.KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# train classifiers\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_test_pred = knn.predict(X_test)\n",
    "\n",
    "accuracy = sklearn.metrics.accuracy_score(y_test, y_test_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "functioning-tribune",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10233, 105)\n",
      "(2559, 105)\n",
      "10233\n",
      "2559\n",
      "[5170 5063]\n",
      "[1274 1285]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "print(len(y_train))\n",
    "print(len(y_test))\n",
    "\n",
    "print(np.unique(y_train, return_counts=True)[1])\n",
    "print(np.unique(y_test, return_counts=True)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "flush-range",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "requested-notice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.6322 - accuracy: 0.6362 - val_loss: 0.6028 - val_accuracy: 0.6494\n",
      "Epoch 2/30\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5661 - accuracy: 0.6995 - val_loss: 0.5757 - val_accuracy: 0.7070\n",
      "Epoch 3/30\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5434 - accuracy: 0.7134 - val_loss: 0.5709 - val_accuracy: 0.6973\n",
      "Epoch 4/30\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5438 - accuracy: 0.7118 - val_loss: 0.5688 - val_accuracy: 0.6953\n",
      "Epoch 5/30\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5142 - accuracy: 0.7352 - val_loss: 0.5537 - val_accuracy: 0.7188\n",
      "Epoch 6/30\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5008 - accuracy: 0.7444 - val_loss: 0.5488 - val_accuracy: 0.7256\n",
      "Epoch 7/30\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4954 - accuracy: 0.7489 - val_loss: 0.5636 - val_accuracy: 0.7109\n",
      "Epoch 8/30\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4954 - accuracy: 0.7463 - val_loss: 0.5797 - val_accuracy: 0.7080\n",
      "Epoch 9/30\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4864 - accuracy: 0.7547 - val_loss: 0.5589 - val_accuracy: 0.7129\n",
      "Epoch 10/30\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4803 - accuracy: 0.7586 - val_loss: 0.5481 - val_accuracy: 0.7119\n",
      "Epoch 11/30\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4721 - accuracy: 0.7619 - val_loss: 0.5519 - val_accuracy: 0.7139\n",
      "Epoch 12/30\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.4729 - accuracy: 0.7640 - val_loss: 0.5675 - val_accuracy: 0.7227\n",
      "Epoch 13/30\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.4645 - accuracy: 0.7624 - val_loss: 0.5603 - val_accuracy: 0.7109\n",
      "Epoch 14/30\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.4541 - accuracy: 0.7740 - val_loss: 0.5724 - val_accuracy: 0.7207\n",
      "Epoch 15/30\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.4523 - accuracy: 0.7746 - val_loss: 0.5604 - val_accuracy: 0.7158\n",
      "Epoch 16/30\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.4440 - accuracy: 0.7789 - val_loss: 0.5732 - val_accuracy: 0.7207\n",
      "Epoch 17/30\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.4395 - accuracy: 0.7836 - val_loss: 0.5790 - val_accuracy: 0.7168\n",
      "Epoch 18/30\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.4349 - accuracy: 0.7851 - val_loss: 0.5781 - val_accuracy: 0.7197\n",
      "Epoch 19/30\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.4313 - accuracy: 0.7828 - val_loss: 0.5872 - val_accuracy: 0.7334\n",
      "Epoch 20/30\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.4253 - accuracy: 0.7968 - val_loss: 0.6090 - val_accuracy: 0.7217\n",
      "Epoch 21/30\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.4185 - accuracy: 0.7925 - val_loss: 0.6149 - val_accuracy: 0.7168\n",
      "Epoch 22/30\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.4185 - accuracy: 0.7981 - val_loss: 0.6094 - val_accuracy: 0.7236\n",
      "Epoch 23/30\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.4133 - accuracy: 0.7973 - val_loss: 0.6148 - val_accuracy: 0.7051\n",
      "Epoch 24/30\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.4059 - accuracy: 0.8033 - val_loss: 0.6538 - val_accuracy: 0.7021\n",
      "Epoch 25/30\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.3952 - accuracy: 0.8065 - val_loss: 0.6307 - val_accuracy: 0.7246\n",
      "Epoch 26/30\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.3909 - accuracy: 0.8136 - val_loss: 0.6360 - val_accuracy: 0.7246\n",
      "Epoch 27/30\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.3880 - accuracy: 0.8122 - val_loss: 0.6578 - val_accuracy: 0.7070\n",
      "Epoch 28/30\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.3863 - accuracy: 0.8103 - val_loss: 0.6727 - val_accuracy: 0.7178\n",
      "Epoch 29/30\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.3836 - accuracy: 0.8138 - val_loss: 0.6414 - val_accuracy: 0.7217\n",
      "Epoch 30/30\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.3774 - accuracy: 0.8202 - val_loss: 0.6782 - val_accuracy: 0.7178\n",
      "The test accuracy is 0.7346619773348965\n"
     ]
    }
   ],
   "source": [
    "model = ks.models.Sequential()\n",
    "model.add(ks.layers.Flatten(input_shape=[shape]))\n",
    "model.add(ks.layers.Dense(256, activation=\"relu\"))\n",
    "model.add(ks.layers.Dense(128, activation=\"relu\"))\n",
    "model.add(ks.layers.Dense(64, activation=\"relu\"))\n",
    "model.add(ks.layers.Dense(32, activation=\"relu\"))\n",
    "model.add(ks.layers.Dense(2, activation=\"softmax\"))\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.fit(X_train, y_train, batch_size=128, epochs=30, validation_split=0.1)\n",
    "test_predictions = np.argmax(model.predict(X_test), axis=1)\n",
    "test_accuracy = metrics.accuracy_score(y_test, test_predictions)\n",
    "print(f\"The test accuracy is {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "embedded-assessment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('new', 1450), ('coronavirus', 1066), ('president', 961), ('trump', 726), ('pandemic', 716), ('state', 646), ('time', 609), ('people', 572), ('city', 564), ('said', 497), ('year', 490), ('york', 479), ('home', 445), ('country', 417), ('american', 410), ('say', 408), ('like', 384), ('make', 370), ('week', 369), ('world', 368), ('life', 358), ('way', 341), ('health', 321), ('help', 309), ('virus', 309), ('case', 303), ('official', 299), ('student', 298), ('work', 289), ('house', 273), ('day', 262), ('business', 258), ('school', 255), ('woman', 253), ('need', 252), ('company', 250), ('look', 241), ('death', 240), ('public', 239), ('outbreak', 234), ('family', 226), ('child', 225), ('change', 223), ('long', 220), ('crisis', 214), ('government', 214), ('million', 213), ('social', 211), ('biden', 208), ('black', 208), ('month', 196), ('political', 195), ('story', 195), ('worker', 195), ('right', 189), ('white', 189), ('want', 187), ('republican', 184), ('party', 180), ('plan', 178), ('come', 176), ('expert', 174), ('united', 173), ('risk', 170), ('america', 168), ('market', 167), ('police', 164), ('federal', 163), ('campaign', 162), ('election', 161), ('democratic', 160), ('face', 158), ('place', 158), ('care', 157), ('national', 156), ('number', 156), ('thing', 155), ('china', 153), ('leader', 151), ('community', 146), ('good', 144), ('news', 144), ('offer', 144), ('lesson', 143), ('response', 143), ('trying', 143), ('know', 142), ('nation', 142), ('administration', 141), ('book', 141), ('online', 139), ('effort', 138), ('group', 138), ('court', 137), ('decade', 137), ('economy', 136), ('patient', 136), ('power', 136), ('series', 136), ('democrat', 135), ('young', 134), ('joe', 132), ('high', 131), ('recent', 130), ('history', 129), ('latest', 129), ('economic', 128), ('end', 127), ('race', 127), ('set', 127), ('spread', 127), ('war', 127), ('play', 126), ('law', 125), ('left', 125), ('season', 125), ('game', 124), ('question', 124), ('think', 124), ('use', 124), ('big', 123), ('job', 122), ('puzzle', 122), ('fear', 121), ('small', 120), ('team', 120), ('study', 119), ('voter', 118), ('protest', 115), ('different', 114), ('challenge', 113), ('food', 113), ('rule', 113), ('going', 112), ('parent', 112), ('resident', 112), ('hospital', 111), ('infection', 111), ('making', 111), ('learn', 110), ('problem', 110), ('working', 110), ('doctor', 109), ('nearly', 109), ('tell', 108), ('industry', 107), ('little', 107), ('better', 106), ('open', 106), ('far', 105), ('feel', 105), ('live', 105), ('return', 105), ('hope', 104), ('medium', 104), ('test', 104), ('turn', 104), ('candidate', 103), ('critic', 103), ('force', 103), ('line', 103), ('best', 102), ('event', 102), ('future', 102), ('lockdown', 101), ('major', 101), ('mask', 101), ('policy', 101), ('star', 101), ('decision', 100), ('died', 100), ('past', 100), ('program', 100), ('space', 100), ('thousand', 100), ('trial', 100), ('global', 99), ('stock', 99), ('moment', 98), ('restaurant', 98), ('senate', 98), ('video', 98), ('debate', 97), ('issue', 97), ('justice', 97), ('medical', 97), ('early', 96), ('mean', 96), ('getting', 95), ('governor', 95), ('scientist', 95), ('try', 95), ('artist', 94), ('le', 94), ('lot', 94), ('presidential', 94), ('college', 93), ('image', 92), ('restriction', 92), ('used', 92), ('money', 91), ('start', 91), ('amid', 90), ('fight', 90), ('record', 90), ('art', 89), ('service', 89), ('called', 88), ('disease', 88), ('security', 88), ('street', 88), ('summer', 88), ('support', 88), ('conversation', 87), ('including', 87), ('night', 87), ('reader', 87), ('officer', 86), ('sale', 86), ('general', 85), ('member', 85), ('experience', 83), ('friend', 83), ('lost', 83), ('office', 83), ('order', 83), ('south', 83), ('washington', 83), ('came', 82), ('deal', 82), ('known', 82), ('man', 82), ('tuesday', 82), ('reopen', 81), ('sander', 81), ('region', 80), ('away', 79), ('data', 79), ('started', 79), ('took', 79), ('california', 78), ('climate', 78), ('hit', 78), ('idea', 78), ('let', 78), ('men', 78), ('cut', 77), ('island', 77), ('lead', 77), ('likely', 77), ('mayor', 77), ('movement', 77), ('run', 77), ('stay', 77), ('stop', 77), ('attack', 76), ('despite', 76), ('hard', 76), ('owner', 76), ('sport', 76), ('taking', 76), ('talk', 76), ('vaccine', 76), ('changed', 75), ('close', 75), ('department', 75), ('kind', 75), ('running', 75), ('second', 75), ('travel', 75), ('using', 75), ('voting', 75), ('chief', 74), ('employee', 74), ('share', 74), ('tale', 74), ('asked', 73), ('coming', 73), ('neighborhood', 73), ('testing', 73), ('vote', 73), ('chinese', 72), ('human', 72), ('player', 72), ('rate', 72), ('research', 72), ('benefit', 71), ('created', 71), ('fan', 71), ('growing', 71), ('impeachment', 71), ('join', 71), ('killed', 71), ('percent', 71), ('personal', 71), ('price', 71), ('property', 71), ('role', 71), ('consider', 70), ('distancing', 70)]\n",
      "The test accuracy for the shallow model on the test set is 0.6550688360450563\n",
      "The test accuracy for the sequential model on the test set is 0.6916145181476846\n"
     ]
    }
   ],
   "source": [
    "# Predicting on the test set:\n",
    "ts = open_and_preprocess(\"test.csv\")\n",
    "ts = process_column(ts, 'abstract', 300, get_pw_from_file('train.csv', 'abstract', 300))\n",
    "\n",
    "labels = ts['is_popular']\n",
    "ts = ts.drop(['is_popular', 'word_count'], axis=1)\n",
    "\n",
    "# SHALLOW MODEL PREDICTION\n",
    "y_test_pred = knn.predict(ts)\n",
    "accuracy = sklearn.metrics.accuracy_score(labels, y_test_pred)\n",
    "print(f\"The test accuracy for the shallow model on the test set is {accuracy}\")\n",
    "\n",
    "# SEQUENTIAL MODEL PREDICTION\n",
    "test_predictions = np.argmax(model.predict(ts), axis=1)\n",
    "test_accuracy = metrics.accuracy_score(labels, test_predictions)\n",
    "print(f\"The test accuracy for the sequential model on the test set is {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-springer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quarterly-resident",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
